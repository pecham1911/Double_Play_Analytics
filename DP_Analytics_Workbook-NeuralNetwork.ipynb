{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Play Analytics\n",
    "\n",
    "(1) Forecast the outcome of games; (2) understand the most important factors that influence the outcome of games.\n",
    "\n",
    "- Most important factors of a win? \n",
    "    - Pitching (start), pitching (reliever/closer), defense, runs/hitting (consistent), homeruns, the park, OBP, other?\n",
    "    - run consistency - can you get at least 3 runs per game x percent of the time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing in packages for EDA, pre-processing, modeling, and visualizations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, plot_confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each year is saved in a separate .txt file\n",
    "file_paths = [\"data/gl2010.txt\", \"data/gl2011.txt\", \"data/gl2012.txt\", \"data/gl2013.txt\", \"data/gl2014.txt\",\n",
    "              \"data/gl2015.txt\", \"data/gl2016.txt\", \"data/gl2017.txt\", \"data/gl2018.txt\", \"data/gl2019.txt\",\n",
    "              \"data/gl2020.txt\", \"data/gl2021.txt\", \"data/gl2022.txt\", \"data/gl2023.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        # Read lines from the file\n",
    "        data = f.readlines()\n",
    "        # Split each line by comma and create a list of lists\n",
    "        data_split = [line.strip().split(\",\") for line in data]\n",
    "        # Create DataFrame from the data\n",
    "        df_initial = pd.DataFrame(data_split)\n",
    "        # Append DataFrame to the list\n",
    "        dfs.append(df_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames in the list\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand what the data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the display width to accommodate more characters per row\n",
    "pd.set_option('display.width', 1000)  # Adjust as needed\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the head of the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values = df.isna().any()\n",
    "nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data for more EDA (easier to use with names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns with minimalinformation at the END of the dataset and individual player, manager, and umpire info\n",
    "# Additional dropping needed--> \n",
    "df.drop(columns=df.columns[77:179], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to something more descriptive\n",
    "new_column_names = {\n",
    "    0: 'date',\n",
    "    1: 'num_games',\n",
    "    2: 'day_of_week',\n",
    "    3: 'team_visiting',\n",
    "    4: 'league_visiting',\n",
    "    5: 'game_num_visiting',\n",
    "    6: 'team_home',\n",
    "    7: 'league_home',\n",
    "    8: 'game_num_home',\n",
    "    9: 'score_visiting',\n",
    "    10: 'score_home',\n",
    "    11: 'outs_in_game',\n",
    "    12: 'time_of_day',\n",
    "    13: 'game_completed',\n",
    "    14: 'forfeit',\n",
    "    15: 'protest',\n",
    "    16: 'park_id',\n",
    "    17: 'attendance',\n",
    "    18: 'length_min',\n",
    "    19: 'line_score_visiting',\n",
    "    20: 'line_score_home',   \n",
    "    \n",
    "    # Offense - Visiting\n",
    "    21: 'at_bats_visiting',\n",
    "    22: 'hits_visiting',\n",
    "    23: 'double_visiting',\n",
    "    24: 'triple_visiting',\n",
    "    25: 'home_run_visiting',\n",
    "    26: 'rbi_visiting',\n",
    "    27: 'sacrifice_hit_visiting',\n",
    "    28: 'sacrifine_fly_visiting',\n",
    "    29: 'hit_by_pitch_visiting',\n",
    "    30: 'walk_visiting',\n",
    "    31: 'intent_walk_visiting',\n",
    "    32: 'strikeout_visiting',\n",
    "    33: 'stolen_base_visiting',\n",
    "    34: 'caught_stealing_visiting',\n",
    "    35: 'grounded_into_double_plays_visiting',\n",
    "    36: 'first_catcher_interfere_visiting',\n",
    "    37: 'left_on_base_visiting',\n",
    "    \n",
    "    # Pitching - Visiting\n",
    "    38: 'pitchers_used_visiting',\n",
    "    39: 'individual_earned_runs_visiting',\n",
    "    40: 'team_earned_runs_visiting',\n",
    "    41: 'wild_pitches_visiting',\n",
    "    42: 'balks_visiting',\n",
    "    \n",
    "    # Defense - Visiting\n",
    "    43: 'putouts_visiting',   \n",
    "    44: 'assists_visiting',\n",
    "    45: 'errors_visiting',\n",
    "    46: 'passed_balls_visiting',\n",
    "    47: 'double_def_visiting',\n",
    "    48: 'triple_def_visiting',   \n",
    "    \n",
    "    # Offense - Home\n",
    "    49: 'at_bats_home',\n",
    "    50: 'hits_home',\n",
    "    51: 'double_home',\n",
    "    52: 'triple_home',\n",
    "    53: 'home_run_home',\n",
    "    54: 'rbi_home',\n",
    "    55: 'sacrifice_hit_home',\n",
    "    56: 'sacrifine_fly_home',\n",
    "    57: 'hit_by_pitch_home',\n",
    "    58: 'walk_home',\n",
    "    59: 'intent_walk_home',\n",
    "    60: 'strikeout_home',\n",
    "    61: 'stolen_base_home',\n",
    "    62: 'caught_stealing_home',\n",
    "    63: 'grounded_into_double_plays_home',\n",
    "    64: 'first_catcher_interfere_home',\n",
    "    65: 'left_on_base_home',\n",
    "    \n",
    "    # Pitching - Home\n",
    "    66: 'pitchers_used_home',\n",
    "    67: 'individual_earned_runs_home',\n",
    "    68: 'team_earned_runs_home',\n",
    "    69: 'wild_pitches_home',\n",
    "    70: 'balks_home',\n",
    "    \n",
    "    # Defense - Home\n",
    "    71: 'putouts_home',\n",
    "    72: 'assists_home',\n",
    "    73: 'errors_home',\n",
    "    74: 'passed_balls_home',\n",
    "    75: 'double_def_home',\n",
    "    76: 'triple_def_home',    \n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns=new_column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting most numeric columns from strings to integers\n",
    "columns_to_convert = [\n",
    "    \"num_games\", \n",
    "    \"game_num_visiting\", \n",
    "    \"game_num_home\", \n",
    "    \"score_visiting\", \n",
    "    'game_num_home', \n",
    "    'score_visiting',    \n",
    "    'score_home', \n",
    "    \"outs_in_game\", \n",
    "    \"attendance\", \n",
    "    \"length_min\", \n",
    "    \"at_bats_visiting\",                      \n",
    "    \"hits_visiting\", \n",
    "    \"double_visiting\", \n",
    "    \"triple_visiting\", \n",
    "    \"home_run_visiting\", \n",
    "    \"rbi_visiting\", \n",
    "    \"sacrifice_hit_visiting\",  \n",
    "    'sacrifine_fly_visiting',\n",
    "    'hit_by_pitch_visiting',\n",
    "    'walk_visiting',\n",
    "    'intent_walk_visiting',\n",
    "    'strikeout_visiting',\n",
    "    'stolen_base_visiting',\n",
    "    'caught_stealing_visiting',\n",
    "    'grounded_into_double_plays_visiting',\n",
    "    'first_catcher_interfere_visiting',\n",
    "    'left_on_base_visiting',\n",
    "    'pitchers_used_visiting',\n",
    "    'individual_earned_runs_visiting',\n",
    "    'team_earned_runs_visiting',\n",
    "    'wild_pitches_visiting',\n",
    "    'balks_visiting',\n",
    "    'putouts_visiting',   \n",
    "    'assists_visiting',\n",
    "    'errors_visiting',\n",
    "    'passed_balls_visiting',\n",
    "    'double_def_visiting',\n",
    "    'triple_def_visiting',       \n",
    "    'at_bats_home',\n",
    "    'hits_home',\n",
    "    'double_home',\n",
    "    'triple_home',\n",
    "    'home_run_home',\n",
    "    'rbi_home',\n",
    "    'sacrifice_hit_home',\n",
    "    'sacrifine_fly_home',\n",
    "    'hit_by_pitch_home',\n",
    "    'walk_home',\n",
    "    'intent_walk_home',\n",
    "    'strikeout_home',\n",
    "    'stolen_base_home',\n",
    "    'caught_stealing_home',\n",
    "    'grounded_into_double_plays_home',\n",
    "    'first_catcher_interfere_home',\n",
    "    'left_on_base_home',\n",
    "    'pitchers_used_home',\n",
    "    'individual_earned_runs_home',\n",
    "    'team_earned_runs_home',\n",
    "    'wild_pitches_home',\n",
    "    'balks_home',\n",
    "    'putouts_home',\n",
    "    'assists_home',\n",
    "    'errors_home',\n",
    "    'passed_balls_home',\n",
    "    'double_def_home',\n",
    "    'triple_def_home']\n",
    "\n",
    "# Player positions are numeric, but are being left as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing quotations around strings--\n",
    "columns_to_convert_strings = [\n",
    "  'date',\n",
    "  'num_games',\n",
    "  'day_of_week',\n",
    "  'team_visiting',\n",
    "  'league_visiting',\n",
    "  'team_home',\n",
    "  'league_home',   \n",
    "  'time_of_day',\n",
    "  'park_id',    \n",
    "  'line_score_visiting',\n",
    "  'line_score_home'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_to_convert_strings] = df[columns_to_convert_strings].applymap(lambda x: x.strip('\"\"') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columns with missing data- \n",
    "I think it's likely I won't be using attendance or length_min- will likely drop full columns\n",
    "For the other three I will likely drop the rows, not a noteable number of missings, no reason to believe there is anything systematic about these missings, but I'll look to confirm, e.g., same season, same team, etc.\n",
    "\n",
    "Initial total rows = 32484 \n",
    "\n",
    "Dropped column\n",
    "- attendance                             31549\n",
    "- length_min                             32446\n",
    "- protest\n",
    "- misc\n",
    "\n",
    "Dropped rows\n",
    "- year = 2020; removed this, but not so sure. If I'm not analyzing by year it feels weird to remove it.\n",
    "- at_bats_visiting                       32479\n",
    "- double_visiting                        32446\n",
    "- triple_visiting                        32446\n",
    "\n",
    "\n",
    "##### Fields with large 'missing' /NA - how to handle these missing? \n",
    " - saving_pitcher_id\n",
    " - saving_pitcher_name\n",
    " - game_winning_rbi_id\n",
    " - game_winning_rbi_name\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values in the listed columns\n",
    "df.dropna(subset=[\"at_bats_visiting\", \"double_visiting\", \"triple_visiting\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row where game ended in a tie; found this obs because when calculated run differential there was one obs with a 0 differential\n",
    "df.drop(index=16957, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where there was a protest\n",
    "df.dropna(subset=['protest'], inplace=True)\n",
    "df = df[(df['protest'] != '\"V\"') & (df['protest'] != '\"H\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This drops the partial 2020 season; not sure I want to drop it. It was an odd year, \n",
    "# but unless I'm looking at year as a factor what is the impact of keeping it in?\n",
    "# df = df[df['year'] != 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"attendance\", \"num_games\", \"length_min\", \"game_completed\", \"forfeit\", \"protest\", \"day_of_week\", \"league_visiting\", \"league_home\", \"park_id\", \"time_of_day\"], inplace=True)\n",
    "# both game_completed and forfeit had zero values/100% null for this timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLO/MIA is the only team that switched three letter codes during this time period. Combining them as MIA\n",
    "df['team_visiting'].replace({\"FLO\": \"MIA\"}, inplace=True)\n",
    "df['team_home'].replace({\"FLO\": \"MIA\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an integer four-digit year feature\n",
    "\n",
    "# Extract the first four characters\n",
    "year_digits = df['date'].str[0:4]\n",
    "\n",
    "# Convert the extracted substring to numeric\n",
    "df['year'] = pd.to_numeric(year_digits)\n",
    "\n",
    "df['year'] = df['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert this back to if else, don't need elif since dropping tie game\n",
    "# Creating a feature for the LOSING team for each game/row\n",
    "def compare_and_get_value(row):\n",
    "    if row['score_visiting'] < row['score_home']:\n",
    "        return row['team_visiting']\n",
    "    elif row ['score_home'] < row['score_visiting']:\n",
    "        return row['team_home']\n",
    "    else:\n",
    "        return 'tie_game'\n",
    "\n",
    "# Create a new feature based on the comparison\n",
    "df['losing_team'] = df.apply(compare_and_get_value, axis=1)\n",
    "\n",
    "# There is one game that ended in a tie, CNC and PIT Sept 29, 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert this back to if else, don't need elif since dropping tie game\n",
    "# Creating a feature for the WINNING team for each game/row\n",
    "def compare_and_get_value(row):\n",
    "    if row['score_visiting'] > row['score_home']:\n",
    "        return row['team_visiting']\n",
    "    elif row ['score_home'] > row['score_visiting']:\n",
    "        return row['team_home']\n",
    "    else:\n",
    "        return 'tie_game'\n",
    "\n",
    "# Create a new feature based on the comparison\n",
    "df['winning_team'] = df.apply(compare_and_get_value, axis=1)\n",
    "\n",
    "# There is one game that ended in a tie, CNC and PIT Sept 29, 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['winning_team'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_run_differential(row):\n",
    "    if row['winning_team'] == row['team_home']:\n",
    "        return row['score_home'] - row['score_visiting']\n",
    "    else:\n",
    "        return row['score_visiting'] - row['score_home']\n",
    "\n",
    "# Apply the function to create a new column 'goal_difference'\n",
    "df['run_differential'] = df.apply(calculate_run_differential, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['run_differential'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows where 'run differential' is 0, i.e., a tie game\n",
    "observation = df[df['run_differential'] == 0]\n",
    "\n",
    "# Print the entire row(s) corresponding to the observation(s) with a goal difference of 0\n",
    "observation\n",
    "\n",
    "# There is one game that ended in a tie, CNC and PIT Sept 29, 2016 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Runs per Game\n",
    "df['total_runs'] = df['score_home'] + df['score_visiting']\n",
    "df['total_runs'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't think this works/makes sense. It is only reporting one team per row still - \n",
    "# Combine 'team_home' and 'team_visiting' columns into a single column\n",
    "#df['teams'] = df['team_home'].append(df['team_visiting'], ignore_index=True)\n",
    "\n",
    "#df['teams'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'team_home' and 'team_visiting' columns into a single column\n",
    "df['strikeouts'] = df['strikeout_home'] + df['strikeout_visiting']\n",
    "\n",
    "df['strikeouts'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking up the season into three groups; do factors of success differ depending on where in the season you are\n",
    "# 1 is beginning, 2 is middle, and 3 is end of season\n",
    "# There are missing data for tertiles (grouped and home and visiting) - need to look at this\n",
    "# This won't work for 2020!\n",
    "\n",
    "#total_games = 162\n",
    "\n",
    "# Calculate tertile boundaries\n",
    "#tertile_boundaries = [0, total_games // 3, (total_games // 3) * 2, total_games]\n",
    "\n",
    "\n",
    "# Function to assign tertile\n",
    "#def assign_segment(game_num, boundaries):\n",
    "#    for i in range(len(boundaries) - 1):\n",
    "#        if boundaries[i] < game_num <= boundaries[i + 1]:\n",
    "#            return i + 1\n",
    "#    return None\n",
    "\n",
    "# Assign tertile for home games\n",
    "#df['tertile_home'] = df['game_num_home'].apply(lambda x: assign_segment(x, tertile_boundaries))\n",
    "\n",
    "# Assign tertile for visiting games\n",
    "#df['tertile_visiting'] = df['game_num_home'].apply(lambda x: assign_segment(x, tertile_boundaries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['tertile'] = df['tertile_home'].append(df['tertile_visiting'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['tertile'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crosstabs, Groupings, and Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique numeric IDs\n",
    "df['ID'] = range(1, len(df) + 1)\n",
    "df['ID'] = df['ID'].astype(str) + '_w'  # Adding suffix '_w' to original IDs\n",
    "\n",
    "# Duplicate rows and modify IDs for duplicates\n",
    "duplicates = df.copy()\n",
    "duplicates['ID'] = duplicates['ID'].apply(lambda x: x.replace('_w', '_l'))  # Changing suffix to '_l'\n",
    "\n",
    "# Concatenate original DataFrame and duplicates\n",
    "result = pd.concat([df, duplicates], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['win'] = result['ID'].apply(lambda x: 1 if x.endswith('_w') else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sorted = result.sort_values(by='ID')\n",
    "result_sorted.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN - test/template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
    "y = (X.sum(axis=1) > 5).astype(int)  # Binary classification\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)  # 10 input features, 50 hidden units\n",
    "        self.fc2 = nn.Linear(50, 2)   # 50 hidden units, 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7029610812664032\n",
      "Epoch 2, Loss: 0.6922645628452301\n",
      "Epoch 3, Loss: 0.6868016362190247\n",
      "Epoch 4, Loss: 0.6812235176563263\n",
      "Epoch 5, Loss: 0.6764574229717255\n",
      "Epoch 6, Loss: 0.6726328253746032\n",
      "Epoch 7, Loss: 0.6686449408531189\n",
      "Epoch 8, Loss: 0.6623287498950958\n",
      "Epoch 9, Loss: 0.6570148348808289\n",
      "Epoch 10, Loss: 0.6515888392925262\n",
      "Epoch 11, Loss: 0.6465299844741821\n",
      "Epoch 12, Loss: 0.6416837990283966\n",
      "Epoch 13, Loss: 0.6361145555973053\n",
      "Epoch 14, Loss: 0.6312559425830842\n",
      "Epoch 15, Loss: 0.6251914799213409\n",
      "Epoch 16, Loss: 0.6213668823242188\n",
      "Epoch 17, Loss: 0.6161412835121155\n",
      "Epoch 18, Loss: 0.6077877402305603\n",
      "Epoch 19, Loss: 0.6008476138114929\n",
      "Epoch 20, Loss: 0.5950348436832428\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.0%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN First Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
    "y = (X.sum(axis=1) > 5).astype(int)  # Binary classification\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)  # 10 input features, 50 hidden units\n",
    "        self.fc2 = nn.Linear(50, 2)   # 50 hidden units, 2 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
